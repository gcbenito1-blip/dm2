# -*- coding: utf-8 -*-
"""DM2_benito_mosada_odato.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dAhRlpChKauiEL_1T3IPL0W07EOFwpom

# Fake News Detection Model Training Pipeline

## Importing Libraries
"""

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import pandas as pd
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_confusion_matrix
from collections import Counter
from nltk.tokenize import word_tokenize
import warnings
from google.colab import files
import kagglehub

warnings.filterwarnings('ignore')

"""## Loading and Preprocessing data
### Dataset: https://www.kaggle.com/datasets/hassanamin/textdb3
"""

# dataset
path = kagglehub.dataset_download("hassanamin/textdb3")
text_data=pd.read_csv(path + "/fake_or_real_news.csv")

#dimension
print("Dataset Shape:", text_data.shape)

#missing values
print("\nMissing Values:")
print(text_data.isnull().sum())
print("\nMissing Values Percentage:")
print((text_data.isnull().sum() / len(text_data) * 100).round(2))

#returns 0, no need for handling missing values

#duplicates
print("\nDuplicate Rows:")
print("Exact duplicates:", text_data.duplicated().sum())
print("Duplicate titles:", text_data['title'].duplicated().sum())
print("Duplicate text content:", text_data['text'].duplicated().sum())

text_data_clean = text_data.drop_duplicates()
text_data_clean = text_data.drop_duplicates(subset=['text'])

#check new dimensions
text_data_clean.shape

"""# Note on handling outliers.
Outliers in text analysis needs additional context before proceeding. Outliers in text analysis is identified using their lengths and we need to be careful about this.

Why do we need to be careful about this:
* Fake news can be very short ("BREAKING: Big news coming!")
* Real news can be very long (detailed investigative reports)
* Removing based solely on length might remove important patterns

For this study, we will ignore handling of outliers and proceed with the model fitting.
"""

# Counting the number of REAL and FAKE values
Counter(text_data_clean['label'])

"""### Splitting data

Splitting the data into training and testing data. The feature that we will use is the 'text' column, for this will be the determining factor for classifying if the article is real or fake, basing on title alone is not enough and combining the title and text produces a lot of noise.

We will be using 80/20 split given the number of our samples, which is 6335.
"""

x_train,x_test,y_train,y_test=train_test_split(text_data_clean['text'],text_data_clean['label'],test_size=0.2)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""### TfidfVectorizer
The TfidfVectorizer function converts a collection of raw documents into a matrix of TF-IDF features.
##### TF (Term Frequency):
* What it measures: How often a word appears in a specific document

* Formula: TF(word, document) = (Number of times word appears in document) / (Total words in document)

* For fake news detection:
  * Fake news might overuse emotional words: "disaster", "urgent", "shocking"
  * Real news uses more measured language: "according to", "study shows", "official report"
  * High TF for sensational words → higher suspicion of fake news

* Example:
  * Fake article: "SHOCKING government conspiracy discovered!" (TF for "shocking" = 1/4 = 0.25)
  * Real article: "Study shows economic growth trends" (TF for "shows" = 1/5 = 0.20)
##### IDF (Inverse Document Frequency):
* What it measures: How rare or important a word is across ALL documents

* Formula: IDF(word) = log(Total documents / Documents containing the word)

* For fake news detection:
  * Common words (low IDF): "the", "and", "is" - appear everywhere, not useful
  * Rare, specific words (high IDF): "hoax", "deep state", "mainstream media" - more distinctive
  * High IDF words are better at distinguishing fake vs real news

* Example with 6,335 samples:
  * Word "the" appears in 6,300 documents: IDF = log(6335/6300) ≈ 0.005 (low importance)
  * Word "hoax" appears in 200 documents: IDF = log(6335/200) ≈ 3.45 (high importance)
"""

tfidf=TfidfVectorizer(stop_words='english', max_df=0.8)
x_train=tfidf.fit_transform(x_train)
x_test=tfidf.transform(x_test)
print(x_test.shape)
print(x_train.shape)

"""## Model Fitting
Training each models, and weighing each algorithm pros and cons.

We have used six models PassiveAggressiveClassifier, GaussianNB, DecisionTreeClassifier, RandomForestClassifier, SVC(Support Vector Classification, also known as radial SVM), LogisticRegression. From these models, we can evaluate each and determine which of these is the most useful model depending on its accuracy

Algorithm Selection Justifications:
1. Passive Aggressive Classifier

    Justification: Specifically designed for text classification and online learning, making it highly effective for streaming text data like news articles

    Strengths: Fast, memory-efficient, and handles high-dimensional sparse text features well

2. Naive Bayes

    Justification: Classic text classification algorithm known for working well with TF-IDF features despite its "naive" independence assumption

    Strengths: Computationally efficient, handles high dimensionality, provides good baseline performance

3. Decision Tree

    Justification: Provides interpretable results and reveals which words/features are most important for classification

    Strengths: Model transparency, no feature scaling needed, handles non-linear relationships

4. Random Forest

    Justification: Ensemble method that improves upon Decision Trees by reducing overfitting while maintaining interpretability through feature importance

    Strengths: Robust to noise, handles high dimensionality, provides feature importance rankings

5. SVM (Support Vector Machine)

    Justification: Effective in high-dimensional spaces like text data, finds optimal decision boundaries between classes

    Strengths: Strong theoretical foundations, works well with TF-IDF features, good for binary classification

6. Logistic Regression

    Justification: Simple, interpretable, and provides probability estimates, widely used as baseline for text classification

    Strengths: Fast training, provides feature coefficients, good performance with regularized text features

Overall Strategy: Combination of simple baselines (Naive Bayes, Logistic Regression) with more sophisticated algorithms (SVM, Random Forest) and specialized text classifiers (Passive Aggressive) to ensure comprehensive model evaluation.
"""

model1=PassiveAggressiveClassifier(max_iter=300)
model1.fit(x_train,y_train)
model2=GaussianNB()
model2.fit(x_train.toarray(),y_train)
model3=DecisionTreeClassifier()
model3.fit(x_train,y_train)
model4=RandomForestClassifier()
model4.fit(x_train,y_train)
model5=SVC()
model5.fit(x_train,y_train)
model6=LogisticRegression()
model6.fit(x_train,y_train)

"""### Testing data Prediction and Evaluation

After training each model, we will now proceed to test each for prediction and evaluate their metrics.
"""

y_pred1=model1.predict(x_test)
y_pred2=model2.predict(x_test.toarray())
y_pred3=model3.predict(x_test)
y_pred4=model4.predict(x_test)
y_pred5=model5.predict(x_test)
y_pred6=model6.predict(x_test)

# models: model1, model2, model3, model4, model5, model6
# predictions: y_pred1, y_pred2, y_pred3, y_pred4, y_pred5, y_pred6

models = {
    'PassiveAggressive': y_pred1,
    'NaiveBayes': y_pred2,
    'DecisionTree': y_pred3,
    'RandomForest': y_pred4,
    'SVM': y_pred5,
    'LogisticRegression': y_pred6
}

# Create evaluation dataframe
results = []

for model_name, y_pred in models.items():
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')  # Use 'binary' if 2 classes
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    results.append({
        'Model': model_name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1
    })

# Convert to DataFrame
results_df = pd.DataFrame(results)
results_df.index = range(1, len(results_df) + 1)
print("Model Performance Comparison:")
print(results_df.round(4))

"""One of the objectives of the study is to evaluate the performance of each model using accuracy, precision, recall, and F1-score. Those are the metrics used.

## VISUALIZATIONS

### Data Visualization using News Title

We will visualize the difference between a fake news and a real news using their title as a metric if we can see some difference.
"""

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))
fig.suptitle('Characters in News Title', fontsize=20)

# Real news
real_news_data = text_data_clean[text_data_clean['label'] == 'REAL']
real_title_lengths = real_news_data['title'].str.len()
ax1.hist(real_title_lengths, color='#66C2A5', linewidth=2, edgecolor='black', alpha=0.8)
ax1.set_title('REAL News', fontsize=15)
ax1.set_xlabel('Title Length (characters)')
ax1.set_ylabel('Frequency')

# Fake news
fake_news_data = text_data_clean[text_data_clean['label'] == 'FAKE']
fake_title_lengths = fake_news_data['title'].str.len()
ax2.hist(fake_title_lengths, color='#FC8D62', linewidth=2, edgecolor='black', alpha=0.8)
ax2.set_title('FAKE News', fontsize=15)
ax2.set_xlabel('Title Length (characters)')
ax2.set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""
### Data Visualization using the News' body

We will now visualize the difference between a fake news and a real news using the New's body content if we can see some difference."""

fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))
fig1.suptitle('Characters in News Text', fontsize=20)

# Real news
real_news_data = text_data_clean[text_data_clean['label'] == 'REAL']
real_text_length= real_news_data['text'].str.len()
ax1.hist(real_text_length, color='#66C2A5', linewidth=2, edgecolor='black', alpha=0.8)
ax1.set_title('REAL News', fontsize=15)
ax1.set_xlabel('News Text(characters)')
ax1.set_ylabel('Frequency')

# Fake news
fake_news_data = text_data_clean[text_data_clean['label'] == 'FAKE']
fake_text_length = fake_news_data['text'].str.len()
ax2.hist(fake_text_length, color='#FC8D62', linewidth=2, edgecolor='black', alpha=0.8)
ax2.set_title('FAKE News', fontsize=15)
ax2.set_xlabel('News Text(characters)')
ax2.set_ylabel('Frequency')

plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 9))
plt.title('Comparing Accuracy of ML Models', fontsize=20, pad=20)

# Pastel color scheme
pastel_colors = ['#FFB3BA', '#FFDFBA', '#FFFFBA', '#BAFFC9', '#BAE1FF', '#E0BBE4']

plt.xticks(fontsize=12, color='black', rotation=45 if len(labels) > 5 else 0)
plt.yticks(fontsize=12, color='black')
plt.ylabel('Accuracy', fontsize=16)
plt.xlabel('Models', fontsize=16)

# Create the bar plot
bars = plt.bar(labels.keys(), labels.values(), edgecolor='black', color=pastel_colors,
               linewidth=1.5, alpha=0.8)

# Add value labels on top of bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=11)

# Improve y-axis to show more detailed ticks
plt.ylim(0, 1.0)  # Set y-axis from 0 to 1 for accuracy
plt.yticks(np.arange(0, 1.1, 0.1))  # Detailed ticks every 0.1

# Add grid for better readability
plt.grid(axis='y', alpha=0.3, linestyle='--')

plt.tight_layout()
plt.show()

"""Calculating confusion matrix to gain insight into the number of false and true negatives and positives."""

cm1=confusion_matrix(y_test,y_pred1)
cm2=confusion_matrix(y_test,y_pred2)
cm3=confusion_matrix(y_test,y_pred3)
cm4=confusion_matrix(y_test,y_pred4)
cm5=confusion_matrix(y_test,y_pred5)
cm6=confusion_matrix(y_test,y_pred6)

print('Confusion Matrix for PassiveAggressiveClassifier')
cm1

print('Confusion Matrix for PassiveAggressiveClassifier')
plot_confusion_matrix(conf_mat=cm1,show_absolute=True,
                                show_normed=True,
                                colorbar=True,class_names=['FAKE','REAL'],
                                cmap='Reds')

print('Confusion Matrix for GaussianNB')
cm2

print('Confusion Matrix for GaussianNB')
plot_confusion_matrix(conf_mat=cm2,show_absolute=True,
                                show_normed=True,
                                colorbar=True,class_names=['FAKE','REAL'],
                                cmap='Oranges')

print('Confusion Matrix for DecisionTreeClassifier')
cm3

print('Confusion Matrix for DecisionTreeClassifier')
plot_confusion_matrix(conf_mat=cm3,show_absolute=True,
                                show_normed=True,
                                colorbar=True,class_names=['FAKE','REAL'],
                                cmap='YlOrBr')

print('Confusion Matrix for RandomForestClassifier')
cm4

print('Confusion Matrix for RandomForestClassifier')
plot_confusion_matrix(conf_mat=cm4,show_absolute=True,
                                show_normed=True,
                                colorbar=True,class_names=['FAKE','REAL'], cmap='Greens')

print('Confusion Matrix for SVC')
cm5

print('Confusion Matrix for SVC')
plot_confusion_matrix(conf_mat=cm5,show_absolute=True,
                                show_normed=True,
                                colorbar=True,class_names=['FAKE','REAL'])

print('Confusion Matrix for LogisticRegression')
cm6

print('Confusion Matrix for LogisticRegression')
plot_confusion_matrix(conf_mat=cm6,show_absolute=True,
                                show_normed=True,
                                colorbar=True,class_names=['FAKE','REAL'], cmap='Purples')